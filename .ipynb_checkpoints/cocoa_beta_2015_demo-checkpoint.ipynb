{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# drawing imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path variables\n",
    "# set paths here and you're good to go...\n",
    "\n",
    "# directory containing coco-a annotations\n",
    "COCOA_DIR = 'data/COCO-A/annotations'\n",
    "# coco-a json file\n",
    "COCOA_ANN = 'cocoa_beta2015.json'\n",
    "# directory containing VisualVerbnet\n",
    "VVN_DIR = '/home/mronchi/Datasets/cocoa/visualVerbNet'\n",
    "# vvn json file\n",
    "VVN_ANN = 'visual_verbnet_beta2015.json'\n",
    "# directory containing the MS COCO images\n",
    "COCO_IMG_DIR = '/home/mronchi/Datasets/coco/images'\n",
    "# directory containing the MS COCO Python API\n",
    "COCO_API_DIR = '/home/mronchi/Libraries/coco/PythonAPI'\n",
    "# directory containing the MS COCO annotations\n",
    "COCO_ANN_DIR = '/home/mronchi/Datasets/coco/annotations'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO-a annotations...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mronchi/Datasets/cocoa/annotations/cocoa_beta2015.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3bd36b831af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0}/{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCOA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCOCOA_ANN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcocoa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mronchi/Datasets/cocoa/annotations/cocoa_beta2015.json'"
     ]
    }
   ],
   "source": [
    "# load cocoa annotations\n",
    "\n",
    "print(\"Loading COCO-a annotations...\")\n",
    "tic = time.time()\n",
    "\n",
    "with open(\"{0}/{1}\".format(COCOA_DIR,COCOA_ANN)) as f:\n",
    "    cocoa = json.load(f)\n",
    "\n",
    "# annotations with agreement of at least 1 mturk annotator\n",
    "cocoa_1 = cocoa['annotations']['1']\n",
    "# annotations with agreement of at least 2 mturk annotator\n",
    "cocoa_2 = cocoa['annotations']['2']\n",
    "# annotations with agreement of at least 3 mturk annotator\n",
    "cocoa_3 = cocoa['annotations']['3']\n",
    "\n",
    "print(\"Done, (t={0:.2f}s).\".format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load visual verbnet\n",
    "\n",
    "print(\"Loading VisualVerbNet...\")\n",
    "tic = time.time()\n",
    "\n",
    "with open(\"{0}/{1}\".format(VVN_DIR,VVN_ANN)) as f:\n",
    "    vvn = json.load(f)\n",
    "\n",
    "# list of 145 visual actions contained in VVN\n",
    "visual_actions = vvn['visual_actions']\n",
    "# list of 17 visual adverbs contained in VVN\n",
    "visual_adverbs = vvn['visual_adverbs']\n",
    "    \n",
    "print(\"Done, (t={0:.2f}s).\".format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual actions in VVN by category\n",
    "\n",
    "# each visual action is a dictionary with the following properties:\n",
    "#  - id:            unique id within VVN\n",
    "#  - name:          name of the visual action\n",
    "#  - category:      visual category as defined in the paper\n",
    "#  - definition:    [empty]\n",
    "#                   an english language description of the visual action\n",
    "#  - verbnet_class: [empty]\n",
    "#                   corresponding verbnet (http://verbs.colorado.edu/verb-index/index.php) entry id for each visual action\n",
    "\n",
    "for cat in set([x['category'] for x in visual_actions]):\n",
    "    print(\"Visual Category: [{0}]\".format(cat))\n",
    "    for va in [x for x in visual_actions if x['category']==cat]:\n",
    "        print(\"\\t - id:[{0}], visual_action:[{1}]\".format(va['id'],va['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual adverbs in VVN by category\n",
    "\n",
    "# each visual adverb is a dictionary with the following properties:\n",
    "#  - id:            unique id within VVN\n",
    "#  - name:          name of the visual action\n",
    "#  - category:      visual category as defined in the paper\n",
    "#  - definition:    [empty]\n",
    "#                   an english language description of the visual action\n",
    "\n",
    "# NOTE: relative_location is the location of the object with respect to the subject.\n",
    "# It is not with respect to the reference frame of the image.\n",
    "# i.e. if you where the subject, where is the object with respect to you?\n",
    "\n",
    "for cat in set([x['category'] for x in visual_adverbs]):\n",
    "    print(\"Visual Category: [{0}]\".format(cat))\n",
    "    for va in [x for x in visual_adverbs if x['category']==cat]:\n",
    "        print(\"\\t - id:[{0}], visual_adverb:[{1}]\".format(va['id'],va['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each annotation in cocoa is a dictionary with the following properties:\n",
    "\n",
    "#  - id:             unique id within coco-a\n",
    "#  - image_id:       unique id of the image from the MS COCO dataset\n",
    "#  - object_id:      unique id of the object from the MS COCO dataset\n",
    "#  - subject_id:     unique id of the subject from the MS COCO dataset\n",
    "#  - visual_actions: list of visual action ids performed by the subject (with the object if present)\n",
    "#  - visual_adverbs: list of visual adverb ids describing the subject (and object interaction if present)\n",
    "print(\"=\"*30)\n",
    "\n",
    "# find all interactions between any subject and any object in an image\n",
    "image_id = 516931\n",
    "image_interactions = [x for x in cocoa_2 if x['image_id']==image_id]\n",
    "print(image_interactions)\n",
    "print(\"=\"*30)\n",
    "\n",
    "# find all interactions of a subject with any object\n",
    "subject_id = 190190\n",
    "# NOTE: In this image there is no interaction with guitar cause it is not annotated in MS COCO\n",
    "subject_interactions = [x for x in cocoa_2 if x['subject_id']==subject_id]\n",
    "print(subject_interactions)\n",
    "print(\"=\"*30)\n",
    "\n",
    "# find interactions of all subjects with an object\n",
    "object_id = 304500\n",
    "object_interactions = [x for x in cocoa_2 if x['object_id']==object_id]\n",
    "print(object_interactions)\n",
    "print(\"=\"*30)\n",
    "\n",
    "# find all interactions containing a certain visual action\n",
    "va_name = 'play_instrument'\n",
    "va_id   = [x for x in visual_actions if x['name']==va_name][0]['id']\n",
    "interactions = [x for x in cocoa_2 if va_id in x['visual_actions']]\n",
    "print(interactions)\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco-a is organized to be easily integrable with MS COCO\n",
    "\n",
    "# load coco annotations\n",
    "ANN_FILE_PATH = \"{0}/instances_{1}.json\".format(COCO_ANN_DIR,'train2014')\n",
    "\n",
    "if COCO_API_DIR not in sys.path:\n",
    "    sys.path.append( COCO_API_DIR )\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO( ANN_FILE_PATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize an image with subject and object\n",
    "# and print the interaction annotations\n",
    "\n",
    "# object_id == -1 means that the annotation is describing a subject and not an interaction\n",
    "interaction  = random.choice([x for x in cocoa_2 if x['object_id']!=-1 if len(x['visual_actions'])>2])\n",
    "image_id     = interaction['image_id']\n",
    "\n",
    "subject_id   = interaction['subject_id']\n",
    "subject_anns = coco.loadAnns(subject_id)[0]\n",
    "\n",
    "object_id    = interaction['object_id']\n",
    "object_anns  = coco.loadAnns(object_id)[0]\n",
    "object_cat   = coco.cats[object_anns['category_id']]['name']\n",
    "\n",
    "v_actions    = interaction['visual_actions']\n",
    "v_adverbs    = interaction['visual_adverbs']\n",
    "\n",
    "print(\"Image ID:  [{0}]\".format(image_id))\n",
    "print(\"Subject ID:[{0}]\".format(subject_id))\n",
    "print(\"Object ID: [{0}], Category: [{1}]\".format(object_id,object_cat))\n",
    "\n",
    "print(\"\\nVisual Actions:\")\n",
    "for va_id in v_actions:\n",
    "    va = [x for x in visual_actions if x['id']==va_id][0]\n",
    "    print(\"  - id:[{0}], name:[{1}]\".format(va['id'],va['name']))\n",
    "    \n",
    "print(\"\\nVisual Adverbs:\")\n",
    "for va_id in v_adverbs:\n",
    "    va = [x for x in visual_adverbs if x['id']==va_id][0]\n",
    "    print(\"  - id:[{0}], name:[{1}]\".format(va['id'],va['name']))\n",
    "\n",
    "img = coco.loadImgs(image_id)[0]\n",
    "I = io.imread(\"{0}/{1}/{2}\".format(COCO_IMG_DIR,'train2014',img['file_name']))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(I)\n",
    "coco.showAnns([subject_anns,object_anns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
